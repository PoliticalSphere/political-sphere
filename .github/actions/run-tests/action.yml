name: 'Run Tests'
description: 'Enterprise-grade test orchestration with coverage reporting, sharding, and result parsing'
author: 'Political Sphere'
branding:
  icon: 'check-circle'
  color: 'green'

# Version: 1.0.0
# Purpose: Orchestrate test execution with comprehensive reporting and artifact management
# Compliance: SEC-01, SEC-02, TEST-01, TEST-02, QUAL-01, QUAL-05, OPS-01, OPS-02

inputs:
  # Core test configuration
  test-type:
    description: 'Type of tests to run: unit, integration, e2e, coverage, api, frontend, shared'
    required: false
    default: 'unit'

  test-command:
    description: 'Custom test command to execute (overrides test-type)'
    required: false
    default: ''

  # Coverage configuration
  coverage-enabled:
    description: 'Enable coverage collection and reporting'
    required: false
    default: 'false'

  coverage-threshold:
    description: 'Minimum coverage percentage required (0-100)'
    required: false
    default: '0'

  coverage-config:
    description: 'Path to coverage configuration JSON file'
    required: false
    default: '.github/actions/run-tests/coverage.config.json'

  # Sharding configuration (for parallel execution)
  shard-index:
    description: 'Current shard index (1-based)'
    required: false
    default: '1'

  shard-total:
    description: 'Total number of shards'
    required: false
    default: '1'

  # Test execution configuration
  timeout-minutes:
    description: 'Test execution timeout in minutes'
    required: false
    default: '15'

  max-workers:
    description: 'Maximum parallel test workers'
    required: false
    default: '2'

  changed-only:
    description: 'Run only tests for changed files (faster CI)'
    required: false
    default: 'false'

  # Environment configuration
  environment:
    description: 'Test environment: development, staging, production, ci'
    required: false
    default: 'ci'

  node-version:
    description: 'Node.js version to use for tests'
    required: false
    default: '20'

  # Artifact and reporting configuration
  upload-results:
    description: 'Upload test results as artifacts'
    required: false
    default: 'true'

  upload-coverage:
    description: 'Upload coverage reports as artifacts'
    required: false
    default: 'true'

  artifact-retention-days:
    description: 'Days to retain test artifacts'
    required: false
    default: '30'

  # Result parsing and annotations
  parse-results:
    description: 'Parse test results and create GitHub annotations'
    required: false
    default: 'true'

  fail-fast:
    description: 'Fail immediately on first test failure'
    required: false
    default: 'false'

  # Retry configuration (for flaky tests)
  retry-failed-tests:
    description: 'Retry failed tests automatically'
    required: false
    default: 'false'

  retry-count:
    description: 'Number of retry attempts for failed tests'
    required: false
    default: '2'

  # Integration with external services
  codecov-token:
    description: 'Codecov token for coverage upload (optional)'
    required: false
    default: ''

  github-token:
    description: 'GitHub token for PR annotations and status checks'
    required: false
    default: '${{ github.token }}'

  # Observability and metrics
  enable-metrics:
    description: 'Enable CloudWatch metrics emission'
    required: false
    default: 'false'

  cloudwatch-namespace:
    description: 'CloudWatch namespace for test metrics'
    required: false
    default: 'PoliticalSphere/CI/Tests'

outputs:
  tests-passed:
    description: 'Boolean indicating if all tests passed'
    value: ${{ steps.run-tests.outputs.tests-passed }}

  tests-run:
    description: 'Total number of tests executed'
    value: ${{ steps.run-tests.outputs.tests-run }}

  tests-failed:
    description: 'Number of tests that failed'
    value: ${{ steps.run-tests.outputs.tests-failed }}

  coverage-percentage:
    description: 'Overall test coverage percentage'
    value: ${{ steps.run-tests.outputs.coverage-percentage }}

  coverage-passed-threshold:
    description: 'Boolean indicating if coverage meets threshold'
    value: ${{ steps.run-tests.outputs.coverage-passed-threshold }}

  result-path:
    description: 'Path to test result JSON file'
    value: ${{ steps.run-tests.outputs.result-path }}

  coverage-path:
    description: 'Path to coverage report directory'
    value: ${{ steps.run-tests.outputs.coverage-path }}

  duration-seconds:
    description: 'Total test execution duration in seconds'
    value: ${{ steps.run-tests.outputs.duration-seconds }}

runs:
  using: 'composite'
  steps:
    - name: Validate inputs
      id: validate-inputs
      shell: bash
      env:
        INPUT_TEST_TYPE: ${{ inputs.test-type }}
        INPUT_COVERAGE_THRESHOLD: ${{ inputs.coverage-threshold }}
        INPUT_SHARD_INDEX: ${{ inputs.shard-index }}
        INPUT_SHARD_TOTAL: ${{ inputs.shard-total }}
        INPUT_TIMEOUT_MINUTES: ${{ inputs.timeout-minutes }}
        INPUT_MAX_WORKERS: ${{ inputs.max-workers }}
        INPUT_RETRY_COUNT: ${{ inputs.retry-count }}
      run: |
        # SEC-01: Input validation for all parameters
        echo "::group::Validating action inputs"

        # Validate test-type
        VALID_TEST_TYPES="unit integration e2e coverage api frontend shared"
        if [[ -n "$INPUT_TEST_TYPE" ]] && ! echo "$VALID_TEST_TYPES" | grep -qw "$INPUT_TEST_TYPE"; then
          echo "::error::Invalid test-type: $INPUT_TEST_TYPE. Must be one of: $VALID_TEST_TYPES"
          exit 1
        fi

        # Validate coverage threshold (0-100)
        if [[ "$INPUT_COVERAGE_THRESHOLD" -lt 0 ]] || [[ "$INPUT_COVERAGE_THRESHOLD" -gt 100 ]]; then
          echo "::error::Invalid coverage-threshold: $INPUT_COVERAGE_THRESHOLD. Must be between 0-100"
          exit 1
        fi

        # Validate shard configuration
        if [[ "$INPUT_SHARD_INDEX" -lt 1 ]] || [[ "$INPUT_SHARD_INDEX" -gt "$INPUT_SHARD_TOTAL" ]]; then
          echo "::error::Invalid shard configuration: index $INPUT_SHARD_INDEX of $INPUT_SHARD_TOTAL"
          exit 1
        fi

        # Validate timeout (1-120 minutes)
        if [[ "$INPUT_TIMEOUT_MINUTES" -lt 1 ]] || [[ "$INPUT_TIMEOUT_MINUTES" -gt 120 ]]; then
          echo "::error::Invalid timeout-minutes: $INPUT_TIMEOUT_MINUTES. Must be between 1-120"
          exit 1
        fi

        # Validate max-workers (1-16)
        if [[ "$INPUT_MAX_WORKERS" -lt 1 ]] || [[ "$INPUT_MAX_WORKERS" -gt 16 ]]; then
          echo "::error::Invalid max-workers: $INPUT_MAX_WORKERS. Must be between 1-16"
          exit 1
        fi

        # Validate retry-count (0-5)
        if [[ "$INPUT_RETRY_COUNT" -lt 0 ]] || [[ "$INPUT_RETRY_COUNT" -gt 5 ]]; then
          echo "::error::Invalid retry-count: $INPUT_RETRY_COUNT. Must be between 0-5"
          exit 1
        fi

        echo "✅ All inputs validated successfully"
        echo "::endgroup::"

    - name: Setup Node.js
      uses: actions/setup-node@60edb5dd545a775178f52524783378180af0d1f8 # v4.0.2
      with:
        node-version: ${{ inputs.node-version }}
        cache: 'npm'

    - name: Install dependencies
      shell: bash
      run: |
        echo "::group::Installing dependencies"
        npm ci --prefer-offline --no-audit
        echo "::endgroup::"

    - name: Run tests
      id: run-tests
      shell: bash
      env:
        TEST_TYPE: ${{ inputs.test-type }}
        TEST_COMMAND: ${{ inputs.test-command }}
        COVERAGE_ENABLED: ${{ inputs.coverage-enabled }}
        COVERAGE_THRESHOLD: ${{ inputs.coverage-threshold }}
        COVERAGE_CONFIG: ${{ inputs.coverage-config }}
        SHARD_INDEX: ${{ inputs.shard-index }}
        SHARD_TOTAL: ${{ inputs.shard-total }}
        TIMEOUT_MINUTES: ${{ inputs.timeout-minutes }}
        MAX_WORKERS: ${{ inputs.max-workers }}
        CHANGED_ONLY: ${{ inputs.changed-only }}
        ENVIRONMENT: ${{ inputs.environment }}
        FAIL_FAST: ${{ inputs.fail-fast }}
        RETRY_FAILED_TESTS: ${{ inputs.retry-failed-tests }}
        RETRY_COUNT: ${{ inputs.retry-count }}
        ENABLE_METRICS: ${{ inputs.enable-metrics }}
        CLOUDWATCH_NAMESPACE: ${{ inputs.cloudwatch-namespace }}
        GITHUB_TOKEN: ${{ inputs.github-token }}
        ACTION_PATH: ${{ github.action_path }}
      run: |
        # OPS-01: Structured logging and error handling
        "$ACTION_PATH/run-tests.sh"

    - name: Parse test results
      id: parse-results
      if: always() && inputs.parse-results == 'true'
      shell: bash
      env:
        RESULT_PATH: ${{ steps.run-tests.outputs.result-path }}
        GITHUB_TOKEN: ${{ inputs.github-token }}
        ACTION_PATH: ${{ github.action_path }}
      run: |
        # QUAL-05: Parse results and create GitHub annotations
        node "$ACTION_PATH/parse-results.mjs"

    - name: Upload test results
      if: always() && inputs.upload-results == 'true'
      uses: actions/upload-artifact@834a144ee995460fba8ed112a2fc961b36a5ec5a # v4.3.6
      with:
        name: test-results-${{ inputs.test-type }}-shard-${{ inputs.shard-index }}
        path: ${{ steps.run-tests.outputs.result-path }}
        retention-days: ${{ inputs.artifact-retention-days }}

    - name: Upload coverage reports
      if: always() && inputs.upload-coverage == 'true' && inputs.coverage-enabled == 'true'
      shell: bash
      env:
        COVERAGE_PATH: ${{ steps.run-tests.outputs.coverage-path }}
        SHARD_INDEX: ${{ inputs.shard-index }}
        RETENTION_DAYS: ${{ inputs.artifact-retention-days }}
      run: |
        # Upload coverage artifacts with proper naming for aggregation
        ${{ github.action_path }}/upload-artifacts.sh

    - name: Upload to Codecov
      if: always() && inputs.coverage-enabled == 'true' && inputs.codecov-token != ''
      uses: codecov/codecov-action@7afa10ed9b269c561c2336fd862446844e0cbf71 # v4.2.0
      with:
        token: ${{ inputs.codecov-token }}
        files: ${{ steps.run-tests.outputs.coverage-path }}/lcov.info
        flags: ${{ inputs.test-type }}-shard-${{ inputs.shard-index }}
        name: codecov-${{ inputs.test-type }}-shard-${{ inputs.shard-index }}
        fail_ci_if_error: false

    - name: Check test status
      if: always()
      shell: bash
      env:
        TESTS_PASSED: ${{ steps.run-tests.outputs.tests-passed }}
        COVERAGE_ENABLED: ${{ inputs.coverage-enabled }}
        COVERAGE_PASSED_THRESHOLD: ${{ steps.run-tests.outputs.coverage-passed-threshold }}
        COVERAGE_PERCENTAGE: ${{ steps.run-tests.outputs.coverage-percentage }}
        COVERAGE_THRESHOLD: ${{ inputs.coverage-threshold }}
      run: |
        # QUAL-01: Fail if tests did not pass
        if [[ "$TESTS_PASSED" != "true" ]]; then
          echo "::error::Tests failed! See logs above for details."
          exit 1
        fi

        # QUAL-05: Fail if coverage below threshold (when enabled)
        if [[ "$COVERAGE_ENABLED" == "true" ]] && [[ "$COVERAGE_PASSED_THRESHOLD" != "true" ]]; then
          echo "::error::Coverage $COVERAGE_PERCENTAGE% below threshold $COVERAGE_THRESHOLD%"
          exit 1
        fi

        echo "✅ All tests passed successfully!"
